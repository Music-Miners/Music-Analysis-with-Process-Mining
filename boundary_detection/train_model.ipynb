{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 22:24:47.288575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 22:24:50.846738: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-21 22:24:50.846928: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-21 22:24:50.846941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 22:24:54.689923: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-21 22:24:54.689969: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-21 22:24:54.690010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (umn-1684688989): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM, Input, Dense, Activation, Embedding, Concatenate, Reshape\n",
    "from keras.layers import RepeatVector, Permute\n",
    "from keras.layers import Multiply, Lambda\n",
    "import keras.backend as K \n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from the Oskar Kolberg's Dataset\n",
    "https://webesac.pcss.pl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"data_kolberg_with_bars\"]\n",
    "for file in files:\n",
    "    with open('{}.pkl'.format(file), 'rb') as handle:\n",
    "        data.extend(pickle.load(handle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19092"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_pieces = len(data)\n",
    "number_of_pieces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove too long or too short phrases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicate the range of the most common lengths of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_data = []\n",
    "durations_data = []\n",
    "bars_data = [] \n",
    "phrases_data = []\n",
    "\n",
    "for piece in data:\n",
    "    notes_data.extend(piece[0])\n",
    "    durations_data.extend(piece[1])\n",
    "    bars_data.extend(piece[2])\n",
    "    phrases_data.extend(piece[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 62, 64, 65, 67, 69, 67, 65, 65, 65, 65, 67, 65, 64, 60, 62, 64, 65, 67, 69, 67, 65, 65, 65, 65, 67, 65, 64, 65, 65, 64, 62, 64, 64, 62, 62, 60, 59, 57, 55, 60, 62, 64, 65, 67, 64, 62, 60, 128]\n",
      "[0.25, 0.25, 0.5, 0.5, 1.0, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.25, 0.25, 1.0, 0.25, 0.25, 0.5, 0.5, 1.0, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.25, 0.25, 1.0, 1.0, 0.5, 0.25, 0.25, 0.5, 0.5, 1.0, 0.5, 0.25, 0.25, 0.5, 0.5, 0.25, 0.25, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]\n",
      "[0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0])\n",
    "print(data[0][1])\n",
    "print(data[0][2])\n",
    "print(data[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14, 5605),\n",
       " (16, 5309),\n",
       " (12, 5131),\n",
       " (13, 4215),\n",
       " (8, 4181),\n",
       " (15, 3808),\n",
       " (10, 3332),\n",
       " (18, 3300),\n",
       " (17, 3220),\n",
       " (11, 2924),\n",
       " (9, 2878),\n",
       " (19, 2362),\n",
       " (7, 2307),\n",
       " (20, 2035),\n",
       " (21, 1337),\n",
       " (6, 1211),\n",
       " (22, 1063)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_length = dict()\n",
    "counter = 0\n",
    "for i in range(len(phrases_data)):\n",
    "    counter+=1\n",
    "    if phrases_data[i] == 1:\n",
    "        if counter in phrases_length.keys():\n",
    "            phrases_length[counter] += 1\n",
    "        else:\n",
    "            phrases_length[counter] = 1\n",
    "        counter = 0\n",
    "\n",
    "phrases_length = sorted(phrases_length.items(), key=lambda x: x[1], reverse=True)\n",
    "phrases_length = [ (length, count) for length, count in phrases_length if count > 0.05 * number_of_pieces]\n",
    "phrases_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove pieces with irregular phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = phrases_length[0][0]\n",
    "lower_bound = min([ length for length, count in phrases_length ])\n",
    "upper_bound = max([ length for length, count in phrases_length ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The most common phrases length ==> the lenght of a sequence\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shortest phrase lenght we consider\n",
    "lower_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The longest phrase lenght we consider\n",
    "upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = []\n",
    "\n",
    "for piece in data:\n",
    "    # Skip pieces with irregular phrases\n",
    "    current_phrase_length = 0\n",
    "    skip = False\n",
    "\n",
    "    for i, digit in enumerate(piece[3]):\n",
    "        current_phrase_length += 1\n",
    "        if digit == 1:\n",
    "            if current_phrase_length < lower_bound or current_phrase_length > upper_bound:\n",
    "                skip = True\n",
    "                break\n",
    "            current_phrase_length = 0\n",
    "    if skip:\n",
    "        continue\n",
    "    else:\n",
    "        pieces.append(piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16809"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_pieces = len(pieces)\n",
    "number_of_pieces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "pieces_train, pieces_test = train_test_split(pieces, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13447, 3362)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pieces_train), len(pieces_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train and test data to pickle files\n",
    "with open('pieces_train.pkl', 'wb') as handle:\n",
    "    pickle.dump(pieces_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('pieces_test.pkl', 'wb') as handle:\n",
    "    pickle.dump(pieces_test, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_input = []\n",
    "durations_input = []\n",
    "bars_input = []\n",
    "phrases_input = []\n",
    "\n",
    "for piece in pieces_train:\n",
    "    notes = piece[0]\n",
    "    notes_input.append(129) # START = 129\n",
    "    notes_input.extend(notes)\n",
    "\n",
    "    durations = piece[1]\n",
    "    durations_input.append(0)\n",
    "    durations_input.extend(durations)\n",
    "\n",
    "    bars = piece[2]\n",
    "    bars_input.append(0)\n",
    "    bars_input.extend(bars)\n",
    "    \n",
    "    phrases = piece[3]\n",
    "    phrases_input.append(0)\n",
    "    phrases_input.extend(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the notes_input: 553633\n",
      "Twenty first elements: [129, 60, 67, 60, 68, 67, 62, 67, 62, 65, 63, 67, 63, 60, 68, 67, 67, 59, 60, 67]\n",
      "Length of the durations_input: 553633\n",
      "Twenty first elements: [0, 0.5, 0.75, 0.25, 0.5, 1.0, 0.5, 0.75, 0.25, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 1.5, 0.5]\n",
      "Length of the phrases_input: 553633\n",
      "Twenty first elements: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the notes_input: {}\".format(len(notes_input)))\n",
    "print(\"Twenty first elements: {}\".format(notes_input[:20]))\n",
    "print(\"Length of the durations_input: {}\".format(len(durations_input)))\n",
    "print(\"Twenty first elements: {}\".format(durations_input[:20]))\n",
    "print(\"Length of the phrases_input: {}\".format(len(phrases_input)))\n",
    "print(\"Twenty first elements: {}\".format(phrases_input[:20]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map durations and notes to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations_to_int = {}\n",
    "int_to_duration = {}\n",
    "notes_to_int = {}\n",
    "int_to_notes = {}\n",
    "phrases_to_int = {}\n",
    "int_to_phrases = {}\n",
    "bars_to_int = {}\n",
    "int_to_bars = {}\n",
    "\n",
    "for index, duration in enumerate(sorted(set(durations_data).union({0}))):\n",
    "    durations_to_int[duration] = index\n",
    "    int_to_duration[index] = duration\n",
    "\n",
    "for index, note in enumerate(sorted(set(notes_data).union({129}))):\n",
    "    notes_to_int[note] = index\n",
    "    int_to_notes[index] = note\n",
    "\n",
    "for index, phrase in enumerate(sorted(set(phrases_data))):\n",
    "    phrases_to_int[phrase] = index\n",
    "    int_to_phrases[index] = phrase\n",
    "\n",
    "for index, bar in enumerate(sorted(set(bars_data))):\n",
    "    bars_to_int[bar] = index\n",
    "    int_to_bars[index] = bar\n",
    "\n",
    "durations_network_input = [ durations_to_int[duration] for duration in durations_input ]\n",
    "notes_network_input = [ notes_to_int[note] for note in notes_input ]\n",
    "bars_network_input = [ bars_to_int[bar] for bar in bars_input ]\n",
    "phrases_network_input = [ phrases_to_int[phrase] for phrase in phrases_input ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.00390625, 0.0625, 0.125, 0.25, 0.375, 0.5, 0.75, 1.0, 1.5, 2.0, 3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "# Before\n",
    "print(sorted(set(durations_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "# After\n",
    "print(sorted(set(durations_network_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 88, 128, 129]\n"
     ]
    }
   ],
   "source": [
    "# Before\n",
    "print(sorted(set(notes_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 44, 45]\n"
     ]
    }
   ],
   "source": [
    "# After\n",
    "print(sorted(set(notes_network_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the notes_input: 553633\n",
      "Twenty first elements: [45, 14, 21, 14, 22, 21, 16, 21, 16, 19, 17, 21, 17, 14, 22, 21, 21, 13, 14, 21]\n",
      "Length of the durations_input: 553633\n",
      "Twenty first elements: [0, 6, 7, 4, 6, 8, 6, 7, 4, 6, 8, 6, 6, 6, 6, 8, 8, 6, 9, 6]\n",
      "Length of the phrases_input: 553633\n",
      "Twenty first elements: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the notes_input: {}\".format(len(notes_network_input)))\n",
    "print(\"Twenty first elements: {}\".format(notes_network_input[:20]))\n",
    "print(\"Length of the durations_input: {}\".format(len(durations_network_input)))\n",
    "print(\"Twenty first elements: {}\".format(durations_network_input[:20]))\n",
    "print(\"Length of the phrases_input: {}\".format(len(phrases_network_input)))\n",
    "print(\"Twenty first elements: {}\".format(phrases_network_input[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_notes = len(notes_to_int)\n",
    "n_durations = len(durations_to_int)\n",
    "n_phrases = len(phrases_to_int)\n",
    "n_bars = len(bars_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct notes: 46\n",
      "Number of distinct durations: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of distinct notes: {}\".format(n_notes))\n",
    "print(\"Number of distinct durations: {}\".format(n_durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.pkl', 'wb') as file:\n",
    "    dictionary = { \"durations_to_int\" : durations_to_int, \"int_to_duration\" : int_to_duration, \"notes_to_int\" : notes_to_int, \"int_to_notes\" : int_to_notes, \"phrases_to_int\" : phrases_to_int, \"int_to_phrases\" : int_to_phrases, \"bars_to_int\" : bars_to_int, \"int_to_bars\" : int_to_bars }\n",
    "    # A new file will be created\n",
    "    pickle.dump(dictionary, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare network input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, durations, bars, phrases, seq_len = 32):\n",
    "\n",
    "    notes_input = []\n",
    "    durations_input = []\n",
    "    bars_input = []\n",
    "    phrases_output = []\n",
    "\n",
    "    for i in range(len(notes) - seq_len):\n",
    "        notes_input.append(notes[i:i + seq_len])\n",
    "        durations_input.append(durations[i:i + seq_len])\n",
    "        bars_input.append(bars[i:i + seq_len])\n",
    "\n",
    "        phrases_output.append(phrases[i + seq_len])\n",
    "\n",
    "    n_patterns = len(notes_input)\n",
    "\n",
    "    notes_input = np.reshape(notes_input, (n_patterns, seq_len))\n",
    "    durations_input = np.reshape(durations_input, (n_patterns, seq_len))\n",
    "    bars_input = np.reshape(bars_input, (n_patterns, seq_len))\n",
    "    network_input = [notes_input, durations_input, bars_input]\n",
    "\n",
    "    phrases_output = np_utils.to_categorical(phrases_output, num_classes=2)\n",
    "    network_output = [phrases_output]\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input, network_output = prepare_sequences(notes_network_input, durations_network_input, bars_network_input, phrases_network_input, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note input\n",
      "[45 14 21 14 22 21 16 21 16 19 17 21 17 14]\n",
      "duration input\n",
      "[0 6 7 4 6 8 6 7 4 6 8 6 6 6]\n",
      "bars input\n",
      "[0 0 0 1 0 1 0 0 1 0 1 0 0 1]\n",
      "phrases_output\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('note input')\n",
    "print(network_input[0][0])\n",
    "print('duration input')\n",
    "print(network_input[1][0])\n",
    "print('bars input')\n",
    "print(network_input[2][0])\n",
    "print('phrases_output')\n",
    "print(network_output[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(n_notes, n_durations, n_bars, n_phrases, embed_size = 100, rnn_units = 256, use_attention = False):\n",
    "\n",
    "    notes_in = Input(shape = (None,))\n",
    "    durations_in = Input(shape = (None,))\n",
    "    bars_in = Input(shape = (None,))\n",
    "\n",
    "    x1 = Embedding(n_notes, embed_size)(notes_in)\n",
    "    x2 = Embedding(n_durations, embed_size)(durations_in)\n",
    "    x3 = Embedding(n_bars, embed_size)(bars_in)\n",
    "\n",
    "    x = Concatenate()([x1,x2,x3])\n",
    "\n",
    "    x = LSTM(rnn_units, return_sequences=True)(x)\n",
    "\n",
    "    if use_attention:\n",
    "\n",
    "        x = LSTM(rnn_units, return_sequences=True)(x)\n",
    "\n",
    "        e = Dense(1, activation='tanh')(x)\n",
    "        e = Reshape([-1])(e)\n",
    "        alpha = Activation('softmax')(e)\n",
    "\n",
    "        alpha_repeated = Permute([2, 1])(RepeatVector(rnn_units)(alpha))\n",
    "\n",
    "        c = Multiply()([x, alpha_repeated])\n",
    "        c = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(rnn_units,))(c)\n",
    "    \n",
    "    else:\n",
    "        c = LSTM(rnn_units)(x)\n",
    "                                    \n",
    "    phrases_out = Dense(n_phrases, activation = 'softmax', name = 'phrase')(c)\n",
    "   \n",
    "    model = Model([notes_in, durations_in, bars_in], [phrases_out])\n",
    "    \n",
    "\n",
    "    if use_attention:\n",
    "        att_model = Model([notes_in, durations_in, bars_in], alpha)\n",
    "    else:\n",
    "        att_model = None\n",
    "\n",
    "\n",
    "    opti = RMSprop(lr = 0.001)\n",
    "    model.compile(loss=['binary_crossentropy', 'binary_crossentropy'], optimizer=opti)\n",
    "\n",
    "    return model, att_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 22:25:01.650977: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 46)     2116        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 46)     598         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 46)     92          ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, None, 138)    0           ['embedding[0][0]',              \n",
      "                                                                  'embedding_1[0][0]',            \n",
      "                                                                  'embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, None, 512)    1333248     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, None, 512)    2099200     ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 1)      513         ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, None)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, None)         0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 512, None)    0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " permute (Permute)              (None, None, 512)    0           ['repeat_vector[0][0]']          \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, None, 512)    0           ['lstm_1[0][0]',                 \n",
      "                                                                  'permute[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 512)          0           ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " phrase (Dense)                 (None, 2)            1026        ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,436,793\n",
      "Trainable params: 3,436,793\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "embed_size = 46\n",
    "rnn_units = 512\n",
    "use_attention = True\n",
    "model, att_model = create_network(n_notes, n_durations, n_bars, n_phrases, embed_size, rnn_units, use_attention)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2215/2215 [==============================] - 625s 280ms/step - loss: 0.2236 - val_loss: 0.2031\n",
      "Epoch 2/30\n",
      "2215/2215 [==============================] - 680s 307ms/step - loss: 0.1810 - val_loss: 0.1722\n",
      "Epoch 3/30\n",
      "2215/2215 [==============================] - 745s 336ms/step - loss: 0.1675 - val_loss: 0.1698\n",
      "Epoch 4/30\n",
      "2215/2215 [==============================] - 714s 322ms/step - loss: 0.1574 - val_loss: 0.1637\n",
      "Epoch 5/30\n",
      "2215/2215 [==============================] - 719s 325ms/step - loss: 0.1448 - val_loss: 0.1627\n",
      "Epoch 6/30\n",
      "2215/2215 [==============================] - 735s 332ms/step - loss: 0.1286 - val_loss: 0.1684\n",
      "Epoch 7/30\n",
      "2215/2215 [==============================] - 718s 324ms/step - loss: 0.1091 - val_loss: 0.1758\n",
      "Epoch 8/30\n",
      "2215/2215 [==============================] - 729s 329ms/step - loss: 0.0894 - val_loss: 0.2033\n",
      "Epoch 9/30\n",
      "2215/2215 [==============================] - 747s 337ms/step - loss: 0.0709 - val_loss: 0.2217\n",
      "Epoch 10/30\n",
      "2215/2215 [==============================] - 706s 319ms/step - loss: 0.0563 - val_loss: 0.2300\n",
      "Epoch 11/30\n",
      "2215/2215 [==============================] - 747s 337ms/step - loss: 0.0455 - val_loss: 0.2495\n",
      "Epoch 12/30\n",
      "2215/2215 [==============================] - 735s 332ms/step - loss: 0.0381 - val_loss: 0.2662\n",
      "Epoch 13/30\n",
      "2215/2215 [==============================] - 730s 330ms/step - loss: 0.0330 - val_loss: 0.2666\n",
      "Epoch 14/30\n",
      "2215/2215 [==============================] - 710s 320ms/step - loss: 0.0285 - val_loss: 0.2801\n",
      "Epoch 15/30\n",
      "2215/2215 [==============================] - 689s 311ms/step - loss: 0.0267 - val_loss: 0.3069\n",
      "Epoch 16/30\n",
      "2215/2215 [==============================] - 729s 329ms/step - loss: 0.0243 - val_loss: 0.3249\n",
      "Epoch 17/30\n",
      " 600/2215 [=======>......................] - ETA: 9:36 - loss: 0.0183"
     ]
    }
   ],
   "source": [
    "weights_folder = \"weights\"\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(\n",
    "    os.path.join(weights_folder, \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.h5\"),\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint2 = ModelCheckpoint(\n",
    "    os.path.join(weights_folder, \"weights.h5\"),\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss'\n",
    "    , restore_best_weights=True\n",
    "    , patience = 10\n",
    ")\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    checkpoint1\n",
    "    , checkpoint2\n",
    "    , early_stopping\n",
    " ]\n",
    "\n",
    "model.save_weights(os.path.join(weights_folder, \"weights.h5\"))\n",
    "model.fit(network_input, network_output\n",
    "          , epochs=30, batch_size=200\n",
    "          , validation_split = 0.2\n",
    "          , callbacks=callbacks_list\n",
    "          , shuffle=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bed620aa74556e6e6bd3b11d1105b690de0e70139218f60bfc1519e47e474149"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
