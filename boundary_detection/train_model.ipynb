{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM, Input, Dense, Activation, Embedding, Concatenate, Reshape\n",
    "from keras.layers import RepeatVector, Permute\n",
    "from keras.layers import Multiply, Lambda\n",
    "import keras.backend as K \n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from the Oskar Kolberg's Dataset\n",
    "https://webesac.pcss.pl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"data_kolberg\"]\n",
    "for file in files:\n",
    "    with open('../data/Oskar Kolberg\\'s Dataset/{}.pkl'.format(file), 'rb') as handle:\n",
    "        data.extend(pickle.load(handle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_pieces = len(data)\n",
    "number_of_pieces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove too long or too short phrases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicate the range of the most common lengths of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_data = []\n",
    "durations_data = []\n",
    "phrases_data = []\n",
    "\n",
    "for piece in data:\n",
    "    notes_data.extend(piece[0])\n",
    "    durations_data.extend(piece[1])\n",
    "    phrases_data.extend(piece[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_length = dict()\n",
    "counter = 0\n",
    "for i in range(len(phrases_data)):\n",
    "    counter+=1\n",
    "    if phrases_data[i] == 1:\n",
    "        if counter in phrases_length.keys():\n",
    "            phrases_length[counter] += 1\n",
    "        else:\n",
    "            phrases_length[counter] = 1\n",
    "        counter = 0\n",
    "\n",
    "phrases_length = sorted(phrases_length.items(), key=lambda x: x[1], reverse=True)\n",
    "phrases_length = [ (length, count) for length, count in phrases_length if count > 0.05 * number_of_pieces]\n",
    "phrases_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove pieces with irregular phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = phrases_length[0][0]\n",
    "lower_bound = min([ length for length, count in phrases_length ])\n",
    "upper_bound = max([ length for length, count in phrases_length ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most common phrases length ==> the lenght of a sequence\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shortest phrase lenght we consider\n",
    "lower_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The longest phrase lenght we consider\n",
    "upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = []\n",
    "\n",
    "for piece in data:\n",
    "    # Skip pieces with irregular phrases\n",
    "    current_phrase_length = 0\n",
    "    skip = False\n",
    "\n",
    "    for i, digit in enumerate(piece[2]):\n",
    "        current_phrase_length += 1\n",
    "        if digit == 1:\n",
    "            if current_phrase_length < lower_bound or current_phrase_length > upper_bound:\n",
    "                skip = True\n",
    "                break\n",
    "            current_phrase_length = 0\n",
    "    if skip:\n",
    "        continue\n",
    "    else:\n",
    "        pieces.append(piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_pieces = len(pieces)\n",
    "number_of_pieces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "pieces_train, pieces_test = train_test_split(pieces, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pieces_train), len(pieces_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train and test data to pickle files\n",
    "with open('../data/Oskar Kolberg\\'s Dataset/train/pieces_train.pkl', 'wb') as handle:\n",
    "    pickle.dump(pieces_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../data/Oskar Kolberg\\'s Dataset/test/pieces_test.pkl', 'wb') as handle:\n",
    "    pickle.dump(pieces_test, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_input = []\n",
    "durations_input = []\n",
    "phrases_input = []\n",
    "\n",
    "for piece in pieces_train:\n",
    "    notes = piece[0]\n",
    "    notes_input.append(129) # START = 129\n",
    "    notes_input.extend(notes)\n",
    "\n",
    "    durations = piece[1]\n",
    "    durations_input.append(0)\n",
    "    durations_input.extend(durations)\n",
    "\n",
    "    phrases = piece[2]\n",
    "    phrases_input.append(0)\n",
    "    phrases_input.extend(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of the notes_input: {}\".format(len(notes_input)))\n",
    "print(\"Twenty first elements: {}\".format(notes_input[:20]))\n",
    "print(\"Length of the durations_input: {}\".format(len(durations_input)))\n",
    "print(\"Twenty first elements: {}\".format(durations_input[:20]))\n",
    "print(\"Length of the phrases_input: {}\".format(len(phrases_input)))\n",
    "print(\"Twenty first elements: {}\".format(phrases_input[:20]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map durations and notes to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations_to_int = {}\n",
    "int_to_duration = {}\n",
    "notes_to_int = {}\n",
    "int_to_notes = {}\n",
    "phrases_to_int = {}\n",
    "int_to_phrases = {}\n",
    "for index, duration in enumerate(sorted(set(durations_data).union({0}))):\n",
    "    durations_to_int[duration] = index\n",
    "    int_to_duration[index] = duration\n",
    "\n",
    "for index, note in enumerate(sorted(set(notes_data).union({129}))):\n",
    "    notes_to_int[note] = index\n",
    "    int_to_notes[index] = note\n",
    "\n",
    "for index, phrase in enumerate(sorted(set(phrases_data))):\n",
    "    phrases_to_int[phrase] = index\n",
    "    int_to_phrases[index] = phrase\n",
    "\n",
    "durations_network_input = [ durations_to_int[duration] for duration in durations_input ]\n",
    "notes_network_input = [ notes_to_int[note] for note in notes_input ]\n",
    "phrases_network_input = [ phrases_to_int[phrase] for phrase in phrases_input ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before\n",
    "print(sorted(set(durations_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After\n",
    "print(sorted(set(durations_network_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before\n",
    "print(sorted(set(notes_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After\n",
    "print(sorted(set(notes_network_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of the notes_input: {}\".format(len(notes_network_input)))\n",
    "print(\"Twenty first elements: {}\".format(notes_network_input[:20]))\n",
    "print(\"Length of the durations_input: {}\".format(len(durations_network_input)))\n",
    "print(\"Twenty first elements: {}\".format(durations_network_input[:20]))\n",
    "print(\"Length of the phrases_input: {}\".format(len(phrases_network_input)))\n",
    "print(\"Twenty first elements: {}\".format(phrases_network_input[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_notes = len(notes_to_int)\n",
    "n_durations = len(durations_to_int)\n",
    "n_phrases = len(phrases_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of distinct notes: {}\".format(n_notes))\n",
    "print(\"Number of distinct durations: {}\".format(n_durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.pkl', 'wb') as file:\n",
    "    dictionary = { \"durations_to_int\" : durations_to_int, \"int_to_duration\" : int_to_duration, \"notes_to_int\" : notes_to_int, \"int_to_notes\" : int_to_notes, \"phrases_to_int\" : phrases_to_int, \"int_to_phrases\" : int_to_phrases }\n",
    "    # A new file will be created\n",
    "    pickle.dump(dictionary, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare network input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, durations, phrases, seq_len = 32):\n",
    "\n",
    "    notes_input = []\n",
    "    durations_input = []\n",
    "    phrases_output = []\n",
    "\n",
    "    for i in range(len(notes) - seq_len):\n",
    "        notes_input.append(notes[i:i + seq_len])\n",
    "        durations_input.append(durations[i:i + seq_len])\n",
    "\n",
    "        phrases_output.append(phrases[i + seq_len])\n",
    "\n",
    "    n_patterns = len(notes_input)\n",
    "\n",
    "    notes_input = np.reshape(notes_input, (n_patterns, seq_len))\n",
    "    durations_input = np.reshape(durations_input, (n_patterns, seq_len))\n",
    "    network_input = [notes_input, durations_input]\n",
    "\n",
    "    phrases_output = np_utils.to_categorical(phrases_output, num_classes=2)\n",
    "    network_output = [phrases_output]\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input, network_output = prepare_sequences(notes_network_input, durations_network_input, phrases_network_input, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('note input')\n",
    "print(network_input[0][0])\n",
    "print('duration input')\n",
    "print(network_input[1][0])\n",
    "print('phrases_output')\n",
    "print(network_output[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(n_notes, n_durations, n_phrases, embed_size = 100, rnn_units = 256, use_attention = False):\n",
    "\n",
    "    notes_in = Input(shape = (None,))\n",
    "    durations_in = Input(shape = (None,))\n",
    "\n",
    "    x1 = Embedding(n_notes, embed_size)(notes_in)\n",
    "    x2 = Embedding(n_durations, embed_size)(durations_in)\n",
    "\n",
    "    x = Concatenate()([x1,x2])\n",
    "\n",
    "    x = LSTM(rnn_units, return_sequences=True)(x)\n",
    "\n",
    "    if use_attention:\n",
    "\n",
    "        x = LSTM(rnn_units, return_sequences=True)(x)\n",
    "\n",
    "        e = Dense(1, activation='tanh')(x)\n",
    "        e = Reshape([-1])(e)\n",
    "        alpha = Activation('softmax')(e)\n",
    "\n",
    "        alpha_repeated = Permute([2, 1])(RepeatVector(rnn_units)(alpha))\n",
    "\n",
    "        c = Multiply()([x, alpha_repeated])\n",
    "        c = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(rnn_units,))(c)\n",
    "    \n",
    "    else:\n",
    "        c = LSTM(rnn_units)(x)\n",
    "                                    \n",
    "    phrases_out = Dense(n_phrases, activation = 'softmax', name = 'phrase')(c)\n",
    "   \n",
    "    model = Model([notes_in, durations_in], [phrases_out])\n",
    "    \n",
    "\n",
    "    if use_attention:\n",
    "        att_model = Model([notes_in, durations_in], alpha)\n",
    "    else:\n",
    "        att_model = None\n",
    "\n",
    "\n",
    "    opti = RMSprop(lr = 0.001)\n",
    "    model.compile(loss=['binary_crossentropy', 'binary_crossentropy'], optimizer=opti)\n",
    "\n",
    "    return model, att_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "rnn_units = 256\n",
    "use_attention = True\n",
    "model, att_model = create_network(n_notes, n_durations, n_phrases, embed_size, rnn_units, use_attention)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = \"weights\"\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(\n",
    "    os.path.join(weights_folder, \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.h5\"),\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint2 = ModelCheckpoint(\n",
    "    os.path.join(weights_folder, \"weights.h5\"),\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss'\n",
    "    , restore_best_weights=True\n",
    "    , patience = 10\n",
    ")\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    checkpoint1\n",
    "    , checkpoint2\n",
    "    , early_stopping\n",
    " ]\n",
    "\n",
    "model.save_weights(os.path.join(weights_folder, \"weights.h5\"))\n",
    "model.fit(network_input, network_output\n",
    "          , epochs=20, batch_size=32\n",
    "          , validation_split = 0.2\n",
    "          , callbacks=callbacks_list\n",
    "          , shuffle=True\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bed620aa74556e6e6bd3b11d1105b690de0e70139218f60bfc1519e47e474149"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
